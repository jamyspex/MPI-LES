{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LES with Nested Grids\n",
    "\n",
    "The aim of this project is to allow higher time and space resolution on parts of the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nested versus piecewise constant \n",
    "\n",
    "Proper nesting means that for part of a domain, a finer grid is used than for the rest of the domain:             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "        if x_n_b < x < x_n_e and y_n_b < x < y_n_e:\n",
    "                dx = DXN\n",
    "                dy = DYN\n",
    "        else:\n",
    "                dx = DXP\n",
    "                dy = DYP\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Piecewise constant means that for part of a domain, a finer grid is used than for the rest of the domain, but as follows:             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "            if x_n_b < x < x_n_e\n",
    "                    dx = DXN\n",
    "            else:\n",
    "                    dx = DXP\n",
    "            if y_n_b < x < y_n_e:        \n",
    "                    dy = DYN\n",
    "            else:\n",
    "                    dy = DYP    \n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The latter approach is much simpler but has some overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import numpy #as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "alpha,kk = numpy.mgrid[0.1:0.9:0.01,1:6:.2]\n",
    "\n",
    "k = 2**kk #)/numpy.log(2)\n",
    "overhead = (2*alpha*(1-alpha)*(k-1))/(1+alpha*alpha*(k*k-1*0))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(alpha,k,overhead,rstride=1,cstride=1,cmap=cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The figure shows the overhead (additional fraction of grid points required) for the parameters $k$ and $\\alpha$, respectively the increase in linear density of the nested grid compared to the original grid, and the linear ratio of the nested portion to the total domain. If we assume, without loss of generality, that the domain is square, and assuming the orginal resp nested domains have a linear size $m_0$ resp $m_1$ and linear grid point count of $k_0$ resp $k_1$ then the definitions for $k$ and $\\alpha$ are:\n",
    "$$\n",
    "k = k_1 / k_0 \\\\\n",
    "\\alpha = m_1 / m_0\n",
    "$$\n",
    "\n",
    "For a typical case where the nested grid would e.g. be 50% of the linear size and have a 10x finer granularity, the overhead would be 17.3%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def overhead(alpha,k):\n",
    "    return 2*alpha*(1-alpha)*(k-1)/(1+alpha*alpha*(k*k-1*0))\n",
    "\n",
    "alpha = 0.5\n",
    "k = 10.0\n",
    "print( 'Overhead: %2.1f%%' % ( 100*overhead(alpha,k) ))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Actual overhead\n",
    "\n",
    "For the actual use case, we have a `12km x 3km` outer domain and a `1km x 4km` inner domain. The outer domain has a resolution of `4m`, the inner `2m`. So we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def overhead_xy(alpha_x,alpha_y,k):\n",
    "    return (alpha_x*(1-alpha_y)+alpha_x*(1-alpha_y))*(k-1)/(1+alpha_x*alpha_y*k*k)\n",
    "m_0_x = 12\n",
    "m_0_y = 3\n",
    "m_1_x = 4\n",
    "m_1_y = 1\n",
    "k = 4/2\n",
    "alpha_x = m_1_x/m_0_x\n",
    "alpha_y = m_1_y/m_0_y\n",
    "print( 'Overhead: %2.1f%%' % ( 100*overhead_xy(alpha_x,alpha_y,k))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Piecewise constant grid for the MPI LES\n",
    "\n",
    "If we assume as a starting point a pwc grid for the MPI version of the LES, this simplifies the code a lot: the nested grid requires interpolation and therefore conditions on the boundaries with the nested subdomain. \n",
    "\n",
    "For a single grid, there are already 9 different conditions; with a nested grid, the total number of conditions would be 3*8+2 = 26 conditions.\n",
    "\n",
    "The pwc grid only requires to set the grid spacing dependent on the MPI process: every process represents a sub-grid with identical dimensions. \n",
    "\n",
    "For example, if the original grid would be 300x300 and we replace the inner 1/3 with a 10x higher resolution, we obtain a 1200x1200 grid: 100 + 10x100 + 100 in each direction.\n",
    "\n",
    "In general, if the original grid is $m_0 \\times n_0$ and the nested grid is $m_1 \\times n_1$, and the resolution increase is $k$, then the new grid size is \n",
    "\n",
    "$$\n",
    "(m_0 - m_1) + k.m_1\\;\\times\\;(n_0 - n_1) + k.n_1\n",
    "$$\n",
    "\n",
    "We can split this into 12x12 subgrids of 100x100. Then the subgrid spacing is determined by the subgrid coordinate:\n",
    "```python\n",
    "        if (i_s == 0 or i_s == 11): \n",
    "            j_sgs = dx\n",
    "        else:\n",
    "            i_sgs = dx/10\n",
    "        if (j_s == 0 or j_s == 11): \n",
    "            j_sgs = dy\n",
    "        else:\n",
    "            j_sgs = dy/10\n",
    "```\n",
    "\n",
    "Or again generalised, assuming the nested grid is placed in the middle of the original domain:\n",
    "\n",
    "$$\n",
    "n_{s,i} = 2.((m_0 - m_1) + k.m_1) / (m_0 - m_1)\\\\\n",
    "n_{s,j} = 2.((n_0 - n_1) + k.n_1) / (n_0 - n_1)\n",
    "$$\n",
    "\n",
    "\n",
    "The subgrid size in this case is\n",
    "\n",
    "$$\n",
    "m_s = ((m_0 - m_1) + k.m_1) / n_{s,i} = (m_0 - m_1) / 2\\\\\n",
    "n_s = ((n_0 - n_1) + k.n_1) / n_{s,j} = (n_0 - n_1) / 2\n",
    "$$\n",
    "\n",
    "If the original grid spacing is $\\Delta x$ then the grid spacing for the nested grid is $\\Delta x/k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Every process is identified by its subgrid coordinates $i_s,j_s$ so that the pwc case only requires adapt the grid spacing for the process and the time step for the process based on the coordinates. \n",
    "\n",
    "### Grid spacing adaptation\n",
    "\n",
    "This is a static operation: when the process starts, it populates the grid spacing arrays based on the subgrid coordinates. This is done in `grid.f95`\n",
    "\n",
    "```fortran\n",
    "        if (isMaster()) then\n",
    "            do i=-1,(ip*procPerCol)+1\n",
    "                dx1Tot(i) = dxgrid\n",
    "            end do\n",
    "        end if\n",
    "        call distribute1DRealRowWiseArray(dx1Tot,dx1, 2, 1, procPerRow)\n",
    "```\n",
    "\n",
    "where `procPerCol` and `dxgrid` are defined in `params_common_sn.f95`, and similar for `dylTot`. So we need `dxgrid_orig` and `dxgrid_nest` in `params_common_sn.f95`.\n",
    "Then we need to add a condition for the start and end of the nested grid:\n",
    "\n",
    "\n",
    "```fortran\n",
    "        if (isMaster()) then\n",
    "            do i=-1,ip*procPerCol+1\n",
    "                if (i>(nested_grid_start_x+2) .and. i<(nested_grid_end_x+2)) then\n",
    "                    dx1Tot(i) = dxgrid_nest\n",
    "                else\n",
    "                    dx1Tot(i) = dxgrid_orig                \n",
    "            end do\n",
    "        end if\n",
    "        call distribute1DRealRowWiseArray(dx1Tot,dx1, 2, 1, procPerRow)\n",
    "```\n",
    "\n",
    "And similar for the y-direction. The main issue here is the values of `nested_grid_start` and `nested_grid_stop`.\n",
    "The nested grid is 2,003 points, the dx1Tot is 4003. The offset starting from `(1,1)` is `(251,276)`.\n",
    "So \n",
    "\n",
    "```fortran\n",
    "    nested_grid_x = 2000 ! 4km\n",
    "    nested_grid_start_x = 251\n",
    "    nested_grid_end_x  = nested_grid_x + nested_grid_start_x\n",
    "    \n",
    "    nested_grid_y = 500 ! 1km\n",
    "    nested_grid_start_y = 276\n",
    "    nested_grid_end_y  = nested_grid_y + nested_grid_start_y\n",
    "    \n",
    "```\n",
    "\n",
    "Then we need to make sure any occurence of `dx1` uses `dx1Tot(i)` and of `dxl` uses  `dxlTot(i)`. We do this by determining `i` based on the rank of the process. This should be a simple array slice:\n",
    "\n",
    "```fortran\n",
    "    dx1_offset = i_s * ip\n",
    "    dx1 = dx1Tot(-1+dx1_offset:ip+1+dx1_offset)\n",
    "```\n",
    "\n",
    "Actually, when considering this it is clear that we don't need `dx1Tot` and `dxlTot`. Instead, we can compute:\n",
    "\n",
    "\n",
    "```fortran\n",
    "            ! i_s is the subgrid coord\n",
    "            do i=-1,ip+1\n",
    "                ii = i_s*ip+i\n",
    "                if (ii>nested_grid_start_x .and. ii< nested_grid_end_x) then\n",
    "                    dx1(i) = dxgrid_nest\n",
    "                else\n",
    "                    dx1(i) = dxgrid_orig                \n",
    "            end do\n",
    "```\n",
    "\n",
    "### Time step adaptation\n",
    "\n",
    "This is a bit more tricky: given the subgrid coordinates we can easily and statically set the time step (`dt_orig` and `dt_nest`, to be defined in  `params_common_sn.f95`) in `set.f95` but it is a bit more complicated to do syncing.\n",
    "\n",
    "For sending/receiving, we need to determine in which grid the source and the destination are.\n",
    "\n",
    "\n",
    "If sender and receiver are both in the nested grid then sync on every time step `dt_nest` otherwise sync on every time step `dt_orig`. \n",
    "\n",
    "So if we have `n` ticks in the nest for 1 tick in the orig, then we need to count `n` ticks in the nest before we can send.\n",
    "In other words, we need to have a counter in nest which gets reset on hitting `dt_orig`, and we sync either on the first or on the last step.\n",
    "\n",
    "This also means that all calls that send/receive must have access to the tick. \n",
    "\n",
    "What we do is keep the tick constant for the orig domain and decrement for the nested domain. So, in the main timeloop, we do as follows:\n",
    "\n",
    "```fortran        \n",
    "        if (inNestedGrid()) then \n",
    "            ticks = ticks+1\n",
    "            if (ticks == dt_nest/dt_orig) ticks = 0\n",
    "        else\n",
    "            ticks = 0\n",
    "        end if\n",
    "```        \n",
    "\n",
    "So we need to change the subroutines `exchangeRealHalos` and `exchangeRealCorners` in `communication_helper_real` to include the tick, and predicate the sending/receiving on it. \n",
    "\n",
    "<!--\n",
    "- To send from a process in the nested domain to a process in the orig domain only happens when `ticks == 0`.\n",
    "- To send from a process in the nested domain to a process in the nested domain happens on all ticks\n",
    "- To send from a process in the orig domain to a process in the nested domain happens on all ticks\n",
    "- To send from a process in the orig domain to a process in the orig domain happens on all ticks\n",
    "\n",
    "- To receive from a process in the orig domain by a process in the nested domain happens when `ticks == 0`\n",
    "- To receive from a process in the nested domain by a process in the orig domain only happens on all ticks\n",
    "- To receive from a process in the nested domain by a process in the nested domain happens on all ticks\n",
    "- To receive from a process in the orig domain by a process in the orig domain happens on all ticks\n",
    "-->\n",
    "\n",
    "```fortran\n",
    "        if (inNested() .and. .not. inNestedGridByRank(commWith) then\n",
    "            if (ticks == 0) then\n",
    "                    ! populate buffer\n",
    "                    MPI_ISend(...)\n",
    "                    MPI_IRecv(...)\n",
    "            end if\n",
    "        else\n",
    "            MPI_ISend(...)/MPI_IRecv(...)\n",
    "        end if     \n",
    "```\n",
    "\n",
    "#### What about `MPI_allreduce()` calls?\n",
    "\n",
    "The `MPI_allreduce` call used in e.g. `getGlobalMaxOf` does not need syncing, so we don't need ticks. \n",
    "\n",
    "#### Subgrid coordinates\n",
    "\n",
    "So the key question is: what about those subgrid coordinates? \n",
    "\n",
    "The LES code uses `procPerCol,procPerRow` and the grid in each proc is of size `ip,jp`. The identifier for each MPI process is `rank` and the total number of processes (the Master is just one of them) is `mpi_size` which I guess is \n",
    "\n",
    "        mpi_size = procPerCol*procPerRow\n",
    "        \n",
    "So the subgrid coordinates become:\n",
    "\n",
    "```fortran\n",
    "    subroutine calcSubgridCoords(procPerRow,rank,i_s,j_s) \n",
    "        integer, intent(In) :: procPerRow, rank\n",
    "        integer, intent(Out) :: i_s,j_s\n",
    "        i_s = rank / procPerRow ! => row, base 0\n",
    "        j_s = rank % procPerRow !=> column, base 0\n",
    "    end subroutine calcSubgridCoords    \n",
    "```\n",
    "\n",
    "Although actually I should use `MPI_Cart_coords`:\n",
    "\n",
    "```fortran\n",
    "    subroutine calcSubgridCoords(rank,i_s,j_s) \n",
    "        integer, intent(In) :: rank\n",
    "        integer, intent(Out) :: i_s,j_s\n",
    "        integer, dimension(2) :: ij_s\n",
    "        integer :: ierr\n",
    "        call MPI_Cart_coords(comm, rank, 2, ij_s, ierr)\n",
    "        i_s = ij_s(1)\n",
    "        j_s = ij_s(2)        \n",
    "    end subroutine calcSubgridCoords    \n",
    "```\n",
    "\n",
    "\n",
    "Then we create a function `inNestedGrid()` which tells us which grid we're in for the current rank, and a function `inNestedGridByRank(rank)` for any rank.\n",
    "\n",
    "To determine if a process with subgrid coordinates `(i_s,j_s)` is inside the nested grid, we define\n",
    "\n",
    "```fortran\n",
    "    i_s_nest_start = nested_grid_start_x / ip\n",
    "    i_s_nest_end = i_s_nest_start + nested_grid_x / ip\n",
    "    j_s_nest_start = nested_grid_start_y / ip\n",
    "    j_s_nest_end = j_s_nest_start + nested_grid_y / ip     \n",
    "```\n",
    "\n",
    "```fortran\n",
    "    pure logical function inNestedGridByRank(rank) result(in_grid)\n",
    "            integer :: rank\n",
    "            integer :: i_s, j_s\n",
    "            call calcSubgridCoords(procPerRow,rank,i_s,j_s) \n",
    "            in_grid = i_s >= i_s_nest_start .and. is <= i_s_nest_end .and j_s >= j_s_nest_start .and. js <= j_s_nest_end\n",
    "    end function inNestedGridByRank\n",
    "```\n",
    "\n",
    "```fortran\n",
    "    pure logical function inNestedGrid() result(in_grid)\n",
    "            in_grid = inNestedGridByRank(rank)\n",
    "    end function inNestedGrid\n",
    "```\n",
    "\n",
    "\n",
    "This very easy for 2 grids. For more grids we need to number them and then have `inGrid()` and `inGridByRank(rank)` return the number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Issues with current LES code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### `bondv1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- In `bondv1`, the routines `gatheraaa` and `gatherbbb` from `communication_helper_real` are called.\n",
    "\n",
    "    - These routines compute  `max(aaa)` resp. `min(bbb)` over the bottom subgrid row. I wonder why that is, should it not be over the full subgrid? \n",
    "        Answer: no, `aaa` and `bbb` are computed only over the bottom row.\n",
    "    - Also, why not do this using `MPI_AllReduce()` on a custom communicator? \n",
    "        Answer: indeed, that would be better\n",
    "    - Why was `getGlobalMaxOf(aaa)` replaced with `gatheraaa`? \n",
    "        Answer: See above\n",
    "    \n",
    "```fortran\n",
    "MPI_AllReduce(send_data, recv_data, send_recv_count, send_recv_datatype, op, comm, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer ::    send_recv_count, send_recv_datatype \n",
    "    integer ::    op, comm, ierror    \n",
    "    \n",
    "MPI_IAllReduce(send_data, recv_data, send_recv_count, send_recv_datatype, op, comm, req, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer ::    send_recv_count, send_recv_datatype \n",
    "    integer ::    op, comm, req, ierror  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### `anime`\n",
    "\n",
    "Subroutine `distributeu(ua, u,ip, jp, kp, ipmax, jpmax, procPerRow)` sends `u` for every process to the master, which aggregates the portions into the global-sized array `ua`. This is OK for a small number of processes and/or small `ip*jp*kp` but is not scalable to large problems.\n",
    "\n",
    "###  `ifdata`\n",
    "\n",
    "Subroutine `distributeifu(ua, ip, jp, kp, ipmax, jpmax, procPerRow)` does the opposite, it sends portions of the large array `ua` to all processes. Again, this is not scalable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- This happens in following routines:\n",
    "    - `ifdata` (called only once): `u`,`v`,`w`,`p`,`usum`,`vsum`,`wsum`,`f`,`g`,`h`,`fold`,`gold`,`hold` \n",
    "    - `anime`, `ifdata_out` (called every 20 time steps, `avetime=20`): same plus `uani`,`vani`,`wani` and `pani` \n",
    "    - `aveflow` (called only once at the end): `aveu`, `avew`, `avev`, `avep`, `aveuu`, `aveww`, `avevv`, `uwfx`\n",
    "    \n",
    "- Other routines that do a similar operation are:\n",
    "    - `feedbfm` (called only once, in `init`): `zbm`\n",
    "    - `bondv1_data24`: seems unused? Answer: no, is crucial and reads inflow data from file for each timestep. But currently unused. To be added back in later.   \n",
    "    \n",
    "    \n",
    "###  `grid`    \n",
    "\n",
    "The routine `grid` is a special case, called only once, and the arrays are 1-D so small, this is OK:\n",
    "        \n",
    "```fortran\n",
    "        call distribute1DRealRowWiseArray(dx1Tot,dx1, 2, 1, procPerRow)\n",
    "        call distribute1DRealRowWiseArray(dxlTot,dxl, 1, 0, procPerRow)\n",
    "        call distribute1DRealColumnWiseArray(dy1Tot, dy1, 1, 1, procPerRow)\n",
    "        call distribute1DRealColumnWiseArray(dylTot, dyl, 1, 0, procPerRow)\n",
    "```    \n",
    "\n",
    "But I think it is unused and we don't need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With regards to the size, every one of these global arrays is `4*ip*jp*kp`. Considering all the above we'd need storage `26` such arrays, so let's round this up to `32` as total memory requirement.\n",
    "\n",
    "Assuming a typical node has 64GB of memory, then each array can be at most 2GB. With `kp=80`, and assume `ip=jp` this means `ip*ip = 2*1024/4/80M` or `ip*ip = 6.4` so `ip < 2.5K`. \n",
    "\n",
    "So it is sufficient that the master is run on a large-memory instance to make this work. \n",
    "\n",
    "- But this requires a change to the current code: we must dynamically allocate memory only on the master node, not on the other nodes. At the moment, all these large arrays are static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even if this is acceptable in terms of scalability, it would surely be better to use `MPI_scatter` and `MPI_gather` for this?\n",
    "\n",
    "```fortran\n",
    "MPI_Scatter(send_data, send_count, send_datatype, recv_data, recv_count, recv_datatype, root, comm, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer, intent(In) ::    send_count, send_datatype, recv_count, recv_datatype, root\n",
    "    integer, intent(InOut) ::    comm, ierror\n",
    "    \n",
    "MPI_IScatter(send_data, send_count, send_datatype, recv_datatype, recv_count,recv_datatype, root, comm, req, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer ::    send_count, send_datatype, recv_count, RECVTYPE, root\n",
    "    integer ::    comm, req, ierror\n",
    "    \n",
    "MPI_Gather(send_data, send_count, send_datatype, recv_data, recv_count, recv_datatype, root, comm, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer ::    send_count, send_datatype, recv_count, recv_datatype, root\n",
    "    integer ::    comm, ierror\n",
    "    \n",
    "MPI_IGather(send_data, send_count, send_datatype, recv_datatype, recv_count,recv_datatype, root, comm, req, ierror)\n",
    "    <type> ::    send_data(*), recv_data(*)\n",
    "    integer ::    send_count, send_datatype, recv_count, RECVTYPE, root\n",
    "    integer ::    comm, req, ierror\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using these functions has the advantage of simplifying the code, for example for `anime` we'd get:\n",
    "    \n",
    "```fortran\n",
    "    count = ip*jp*kp\n",
    "    call MPI_Gather(per_proc_array, count, MPI_REAL, glob_array, count, MPI_REAL, master_rank, communicator, ierror)\n",
    "```\n",
    "\n",
    "And for `ifdata` we'd get:\n",
    "\n",
    "```fortran\n",
    "    count = ip*jp*kp\n",
    "    call MPI_Scatter(per_proc_array, count, MPI_REAL, glob_array, count, MPI_REAL, master_rank, communicator, ierror)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This will work if the rank of the processes reflects a cartesian grid, and as we use `MPI_Cart_Create` this should be the case. \n",
    "\n",
    "Furthermore, there are the handy routines `MPI_Cart_coords` and `MPI_Cart_rank`:\n",
    "\n",
    "```fortran\n",
    "!MPI_Cart_coords: Determines process coords in Cartesian topology given rank in group.\n",
    "\n",
    "MPI_Cart_coords(comm, rank, maxdims, coords, ierr)\n",
    "    integer  ::  comm, rank, maxdims, coords(*), ierr\n",
    "\n",
    "!MPI_Cart_rank: Determines process rank in communicator given Cartesian location.\n",
    "\n",
    "MPI_Cart_rank(comm, coords, rank, ierr)\n",
    "    integer ::    comm, coords(*), rank, ierr\n",
    "```\n",
    "\n",
    "Finally, for the purpose of e.g. `gatheraaa`, we need a new communicator. The easiest way to create one is I guess using `MPI_Comm_split` with `color = rank / row_size` and `key = rank`\n",
    "\n",
    "```fortran\n",
    "        call MPI_Comm_split(comm, color, world_rank, row_comm,ierr)\n",
    "```\n",
    "\n",
    "This will return a communicator `row_comm` to each calling process. We us this communicator for the bottom row.\n",
    "\n",
    "`MPI_Comm_create_group` must be called by all processes in group, which is a subgroup of the group of comm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MPI on MacOs\n",
    "\n",
    "- MacPorts: Install OpenMPI for gfortran 4.9\n",
    "    \n",
    "        $ port install openmpi-gcc49 +fortran\n",
    "        \n",
    "        \n",
    "          openmpi-gcc49 has the following notes:\n",
    "            The mpicc wrapper (and friends) are installed as:\n",
    "\n",
    "              /opt/local/bin/mpicc-openmpi-gcc49 (likewise mpicxx, ...)\n",
    "\n",
    "            To make openmpi-gcc49's wrappers the default (what you get when\n",
    "            you execute 'mpicc' etc.) please run:\n",
    "\n",
    "              sudo port select --set mpi openmpi-gcc49-fortran     \n",
    "              \n",
    "- MacPorts: Install NetCDF for gfortran 4.9              \n",
    "              \n",
    "        $ port install netcdf-fortran +gcc49                      \n",
    "\n",
    "          netcdf has the following notes:\n",
    "            As of version 4.2 c++ and fortran interfaces are separate ports, netcdf-cxx and netcdf-fortran, respectively.              \n",
    "              \n",
    "- Changes in `Sconstruct`:\n",
    "\n",
    "```python\n",
    "        if OSX==0:    \n",
    "            envF.Append(LIBS=['netcdf']) # for version less than 4.2.0\n",
    "        else:\n",
    "        #WV: OpenMPI puts the Fortran functionality in libmpi_mpifh.dylib\n",
    "            envF.Append(LIBS=['mpi_mpifh','netcdff']) # for version more than and equal to 4.2.0\n",
    "            \n",
    "        if OSX == 1:\n",
    "        # Assuming MacPorts\n",
    "        #WV: For OpenMPI, the module mpi.mod is in the /lib/ folder, so I have to add this to the INCLPATH\n",
    "            INCLPATH = ['/opt/local/include','/opt/local/include/openmpi-gcc49/','/opt/local/lib/openmpi-gcc49/']\n",
    "            LIBPATH = ['/opt/local/lib','/opt/local/lib/openmpi-gcc49/']\n",
    "```\n",
    "\n",
    "- To build the code:\n",
    "\n",
    "        [src]$ scons ocl=0 mpi=1 procPerRow=4 procPerCol=4\n",
    "        \n",
    "- And to run this:\n",
    "\n",
    "        [src]$ mpiexec-openmpi-gcc49 -np 16 ./les_main_mpi\n",
    "        \n",
    "- I also installed MPICH for gfortran-7 but haven't tested it, nor installed the corresponding NetCDF        \n",
    "\n",
    "        $ port install mpich-gcc7 +fortran\n",
    "        \n",
    "          mpich-gcc7 has the following notes:\n",
    "            +--- MPICH Usage note ----\n",
    "            | The mpicc wrapper (and friends) are installed as:\n",
    "            |\n",
    "            |   /opt/local/bin/mpicc-mpich-gcc7 (likewise mpicxx, ...)\n",
    "            |\n",
    "            | To make mpich-gcc7's wrappers the default (what you get when\n",
    "            | you execute 'mpicc' etc.) please run:\n",
    "            |\n",
    "            |   sudo port select --set mpi mpich-gcc7-fortran\n",
    "            +-------------------------\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nested grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As explained above, a nested grid requires interpolation and therefore conditions on the boundaries with the nested subdomain.\n",
    "For a single grid, there are already 9 different conditions; with a nested grid, the total number of conditions would be 3*8+2 = 26 conditions.\n",
    "\n",
    "However, we can make two simplifying assumptions:\n",
    "\n",
    "1. The outer grid also performs the computations in the area covered by the nested grid. \n",
    "2. The outer grid is unaware of the nested grid.\n",
    "\n",
    "In this way there are only 2*9 = 18 conditions to check, of which 9 are already done for the boundaries in the original grid.\n",
    "\n",
    "The nested grid gets its boundary values via interpolation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have an outer cartesian communicator with $m_o \\times n_o$ processes and $k_o \\times l_o$ points per process;\n",
    "the spacing is $dx_o, dy_o$ and the offset is $x_{off,o}, y_{off,o}$\n",
    "\n",
    "We have an inner cartesian communicator with $m_i \\times n_i$ processes and $k_i \\times l_i$ points per process;\n",
    "the spacing is $dx_i, dy_i$ and the offset is $x_{off,i}, y_{off,i}$\n",
    "We assume a boundary, so the actual arrays are larger with the number of boundary points, see the code for details.\n",
    "\n",
    "The inner grid coordinate arrays for subgrid coords $i_i, j_i$:\n",
    "\n",
    "We assume origin is top left\n",
    "\n",
    "    (0,0)\n",
    "        top-left corner = x_off_i, y_off_i\n",
    "        top row =  x_off_i+dx_i*[b,1 .. k_i,b] +dx_i*k_i*i_i   , y_off_i \n",
    "        left col = x_off_i, y_off_i + dy_i *[b, 1 .. l_i, b] +dy_i*l_i*j_i\n",
    "\n",
    "    (0,m_i-1)\n",
    "        top-right corner, top, right\n",
    "\n",
    "    (n_i-1,0)\n",
    "        bottom-left corner, bottom, left\n",
    "\n",
    "    (n_i-1,m_i-1)\n",
    "        bottom-right corner, bottom, right\n",
    "\n",
    "    (i_i, 0)\n",
    "        left col\n",
    "\n",
    "    (i_i, m_i-1)\n",
    "        right col\n",
    "\n",
    "    (0, j_i)\n",
    "        top row\n",
    "\n",
    "    (n_i-1, j_i)\n",
    "        bottom row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "## Outer grid\n",
    "# The outer grid is 800 x 800 points spanning 32,000 x 32,000 \n",
    "x_off_o = 0\n",
    "y_off_o = 0\n",
    "dx_o = 40\n",
    "dy_o = 40\n",
    "# subgrid dimensions\n",
    "m_o = 8\n",
    "n_o = 8\n",
    "# points per process\n",
    "k_o = 100 \n",
    "l_o = 100\n",
    "\n",
    "## Inner grid\n",
    "# The inner grid is 1600 x 1600 points spanning 8,000 x 8,000 \n",
    "x_off_i = 1600\n",
    "y_off_i = 1600\n",
    "dx_i = 5\n",
    "dy_i = 5\n",
    "# subgrid dimensions\n",
    "m_i = 16\n",
    "n_i = 16\n",
    "# \n",
    "# points per process\n",
    "k_i = 100\n",
    "l_i = 100\n",
    "\n",
    "# Let's assume for simplicity that the boundary is uniform 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The question is now: given subgrid coords $i_i$ and $j_i$, what are the subgrid coords $i_o, j_o$ required to do interpolation?\n",
    "\n",
    "I wonder if it is a valid simplification to assume that the inner grid is aligned to the outer grid and a nice fitting multiple?\n",
    "Because in that case we have always a simpler relationship\n",
    "If the inner grid is `100*16*5 = 8,000` and the outer grid is `100*40 per process ` then the outer grid needs `8,000/(40*100)` = 2 processes to cover the same area as the 16 inner processes. \n",
    "If we can do this then we only need to access a single outer process for each inner process except the corners. \n",
    "This is because we don't need to interpolate the overlaps: we can copy these from the inner grid, all this takes is that we first copy the interpolated values from the outer grid and then copy the boundaries from the neighbouring subgrid cell.\n",
    "\n",
    "In any case, with these assumptions, we can calculate the number of inner processes per outer process as:\n",
    "\n",
    "        m_p_o = dx_o*m_o  / (k_i*dx_i)\n",
    "        n_p_o = dy_o*n_o / (l_i*dy_i)                \n",
    "        \n",
    "We can calculate the \"offset\" for the outer processes:\n",
    "\n",
    "        dm_p_o = x_off_i /(dx_o* m_o) # 1600/320 = 5\n",
    "        dn_p_o = y_off_i /(dy_o* n_o) # 1600/320 = 5\n",
    "\n",
    "This means that the subgrid coord for the outer process that covers an inner process is:\n",
    "\n",
    "        i_o = (i_i / m_p_o) + dm_p_o               \n",
    "\n",
    "However, I think this is not valid. Instead, what we need to do is:\n",
    "\n",
    "- determine the enclosing outer points\n",
    "- determine the rank of the process they're in\n",
    "- then gather the enclosing points; note that the number can vary between 4 and 6\n",
    "- then interpolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Inner processes per outer process\n",
    "m_p_i = math.floor(dx_o*k_o  / (k_i*dx_i))\n",
    "n_p_i = math.floor(dy_o*l_o / (l_i*dy_i))\n",
    "\n",
    "# Outer processes covering the inner grid\n",
    "m_p_o = math.floor(k_i*m_i*dx_i/(k_o*dx_o))\n",
    "n_p_o = math.floor(l_i*n_i*dy_i/(l_o*dy_o))\n",
    "\n",
    "\n",
    "#We can calculate the \"offset\" for the outer processes:\n",
    "dm_p_o = math.floor(x_off_i /(dx_o* m_o)) # 1600/320 = 5\n",
    "dn_p_o = math.floor(y_off_i /(dy_o* n_o)) # 1600/320 = 5\n",
    "# print(dm_p_o)\n",
    "\n",
    "#This means that the subgrid coord for the outer process that covers an inner process is:\n",
    "\n",
    "def outer_subgrid_coord_i(i_i):\n",
    "    i_o = math.floor((i_i / m_p_i) + dm_p_o)\n",
    "    return i_o\n",
    "\n",
    "def outer_subgrid_coord_j(j_i):\n",
    "    j_o = math.floor((j_i / n_p_i) + dn_p_o)\n",
    "    return j_o\n",
    "\n",
    "# For the top and bottom rows\n",
    "# Assuming j_i = 0\n",
    "for i_i in range(0,m_i-1):\n",
    "    i_o = outer_subgrid_coord_i(i_i)\n",
    "    print(i_i,i_o,dm_p_o-1)\n",
    "# Assuming j_i = n_i-1 then we need to calculate the subgrid coord for the processes on the other side of the inner grid\n",
    "    print(i_i,i_o,dm_p_o+m_p_o)\n",
    "    \n",
    "# For the leftmost and rightmost columns    \n",
    "# Assuming i_i = 0\n",
    "for j_i in range(0,n_i-1):\n",
    "    j_o = outer_subgrid_coord_j(j_i)\n",
    "    print(j_i,j_o,dn_p_o-1)\n",
    "# Assuming i_i = m_i-1 then we need to calculate the subgrid coord for the processes on the other side of the inner grid\n",
    "    print(j_i,j_o,dn_p_o+n_p_o)    \n",
    "    \n",
    "# For the corners\n",
    "(\n",
    "# top-left corner\n",
    "(dm_p_o-1, dn_p_o-1),\n",
    "# top-right corner\n",
    "(dm_p_o+m_p_o, dn_p_o-1),\n",
    "# bottom-left corner\n",
    "(dm_p_o-1, dn_p_o+n_p_o),\n",
    "# bottom-right corner\n",
    "(dm_p_o+m_p_o, dn_p_o+n_p_o)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the above we have the subgrid coordinates for the processes in the outer grid, and we can get the corresponding rank using `MPI_Cart_rank`.\n",
    "\n",
    "From this process we receive the interpolated array. I guess the best way to do this is to create a communicator for each set of (outer, inners) processes, and do a scatter of the interpolated array. For the given example this would require 8 extra communicators. \n",
    "\n",
    "But first we must do the interpolation. This is very simple: we have an array of 2 points, at a distance `dx_i` or `dy_i` from the side. So we create this array if a process is a neighbour of the subgrid. This we find by computing the intersection with the inner subgrid coords for the bounding box of the outer process:\n",
    "\n",
    "(i_o,j_o) ; dx_o,dy_o; x_off_o, y_off_o => 4 bounding box values: x_l_o, x_r_o, y_b_o, y_t_o\n",
    "\n",
    "<!-- CHECK FOR OFF-BY-1 -->\n",
    "\n",
    "        x_l_o = x_off_o+i_o*dx_o*k_o\n",
    "        x_r_o = x_off_o+i_o*dx_o*k_o+dx_o*(k_o-1)\n",
    "\n",
    "if any of these coincides with the inner bounding box (x_l_i, x_r_i, y_t_i, y_b_i), then it's a neighbour:\n",
    "\n",
    "        x_l_o == x_r_i\n",
    "        x_r_o == x_l_i\n",
    "        y_t_o == y_b_i\n",
    "        y_b_o == y_t_i\n",
    "\n",
    "\n",
    "\n",
    "interpolate using a slice of 2 rows/cols from the outer process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Actual Work Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Use `Scatter`/`Gather` for `aveflow` and `anime`\n",
    "2. Use `AllReduce` for `gatheraaa` and `gatherbbb`\n",
    "3. Implement the timestep-nesting logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### STATUS \n",
    "\n",
    "#### 2017-07-19\n",
    "\n",
    "- `anime`: \n",
    "    * subroutine `anime`: DONE \n",
    "    * subroutine `ifdata_out`: TODO\n",
    "- `aveflow`: \n",
    "    - TODO for all\n",
    "- `ifdata`:\n",
    "    * subroutine `ifdata`: \n",
    "        - DONE for u,v,w,p; \n",
    "        - TODO for f,g,h,usum,vsum,wsum,fold,golf,hold\n",
    "- `gatheraaa`,`gatherbbb`: DONE\n",
    "\n",
    "#### 2017-07-25\n",
    "\n",
    "- Nested LES works in two different modes: \n",
    "    * Uniform time step (see tag `uniform-timestep` in `devel` branch): the grid is stretched to provide higher spatial resolution. The simulation runs at the time step of the nested grid.\n",
    "    * Different time step per grid region (aka 'proper' nesting): the nested part runs with a faster time step than the  original part. The time step ratio is the spatial ratio.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I have implemented the following nesting functionality, all of it guarded with `#ifdef NESTED_LES`:\n",
    "\n",
    "- A helper module `nesting_support` with a global variable `syncTicks` to synchronise the time between the original and nested grid:\n",
    "\n",
    "```fortran\n",
    "module nesting_support\n",
    "    use params_common_sn\n",
    "    use communication_helper_mpi\n",
    "    integer :: syncTicks\n",
    "    save syncTicks\n",
    "    implicit none\n",
    "contains\n",
    "    subroutine calcSubgridCoords(rank,i_s,j_s)\n",
    "        integer, intent(In) :: rank\n",
    "        integer, intent(Out) :: i_s,j_s\n",
    "        integer, dimension(2) :: ij_s\n",
    "        integer :: ierr\n",
    "        call MPI_Cart_coords(cartTopComm, rank, 2, ij_s, ierr)\n",
    "        i_s = ij_s(1)\n",
    "        j_s = ij_s(2)\n",
    "    end subroutine calcSubgridCoords\n",
    "\n",
    "    subroutine currentSubgridCoords(i_s,j_s)\n",
    "        integer, intent(Out) :: i_s,j_s\n",
    "        integer, dimension(2) :: ij_s\n",
    "        integer :: ierror, rank\n",
    "        call MPI_COMM_Rank(communicator, rank, ierror)\n",
    "        call checkMPIError()\n",
    "        call MPI_Cart_coords(cartTopComm, rank, 2, ij_s, ierror)\n",
    "        call checkMPIError()\n",
    "        i_s = ij_s(1)\n",
    "        j_s = ij_s(2)\n",
    "    end subroutine currentSubgridCoords\n",
    "\n",
    "    logical function inNestedGridByRank(rank) result(in_grid)\n",
    "            integer, intent(In) :: rank\n",
    "            integer :: i_s, j_s\n",
    "            call calcSubgridCoords(rank,i_s,j_s)\n",
    "            in_grid = i_s >= i_s_nest_start .and. i_s <= i_s_nest_end .and. j_s >= j_s_nest_start .and. j_s <= j_s_nest_end\n",
    "    end function inNestedGridByRank\n",
    "\n",
    "    logical function inNestedGrid() result(in_grid)\n",
    "            integer :: rank ,ierror\n",
    "            call MPI_COMM_Rank(communicator, rank, ierror)\n",
    "            call checkMPIError()\n",
    "            in_grid = inNestedGridByRank(rank)\n",
    "    end function inNestedGrid\n",
    "\n",
    "end module nesting_support\n",
    "```\n",
    "\n",
    "- Code to set the `syncTicks` in the timeloop in `main`:\n",
    "\n",
    "```fortran\n",
    "        if (inNestedGrid()) then\n",
    "            time = float(n-1)*dt_nest\n",
    "            syncTicks = syncTicks+1\n",
    "            if (syncTicks == dt_nest/dt_orig) syncTicks = 0\n",
    "        else\n",
    "            time = float(n-1)*dt_orig\n",
    "            syncTicks = 0\n",
    "        end if\n",
    "```\n",
    "\n",
    "- Code to guard halo transfers using `syncTicks` in `exchangeRealHalos` and `exchangeRealHaloCorners` in `communication_helper_real`:\n",
    "\n",
    "```fortran\n",
    "        if ( (inNested() .and. .not. inNestedGridByRank(commWith) .and. (syncTicks == 0) ) .or. .not. (inNested() .and. .not. inNestedGridByRank(commWith))) then\n",
    "\n",
    "    ...\n",
    "\n",
    "        else\n",
    "            ! Skip the transfer, set the request status accordingly\n",
    "            requests(1) = MPI_REQUEST_NULL\n",
    "            requests(2) = MPI_REQUEST_NULL\n",
    "        end if\n",
    "```\n",
    "\n",
    "- Code for defining the nested grid in `params_common_sn`:\n",
    "\n",
    "```fortran\n",
    "! Nested grid location and size\n",
    "    integer, parameter :: nested_grid_x = 2000 ! 4km\n",
    "    integer, parameter :: nested_grid_start_x = 251\n",
    "    integer, parameter :: nested_grid_end_x  = nested_grid_x + nested_grid_start_x\n",
    "\n",
    "    integer, parameter :: nested_grid_y = 500 ! 1km\n",
    "    integer, parameter :: nested_grid_start_y = 276\n",
    "    integer, parameter :: nested_grid_end_y  = nested_grid_y + nested_grid_start_y\n",
    "\n",
    "! Nest grid resolution\n",
    "    real, parameter :: dxgrid_nest = 2.0\n",
    "    real, parameter :: dygrid_nest = 2.0\n",
    "    real, parameter :: dxgrid_orig = 4.0\n",
    "    real, parameter :: dygrid_orig = 4.0\n",
    "\n",
    "! Subgrid coordinates for nest\n",
    "    integer, parameter :: i_s_nest_start =  nested_grid_start_x / ip\n",
    "    integer, parameter :: i_s_nest_end =  i_s_nest_start + nested_grid_x / ip\n",
    "    integer, parameter :: j_s_nest_start =  nested_grid_start_y / ip\n",
    "    integer, parameter :: j_s_nest_end =  j_s_nest_start + nested_grid_y / ip\n",
    "\n",
    "! Time steps\n",
    "    real, parameter :: dt_nest = 0.025 ! seconds\n",
    "    real, parameter :: dt_orig = 0.05 ! seconds\n",
    "```\n",
    "\n",
    "- Code for defining the nested grid in `grid`:\n",
    "\n",
    "```fortran\n",
    "    call currentSubgridCoords(i_s,j_s)\n",
    "    do i=-1,ip+1\n",
    "        ii = i_s*ip+i\n",
    "        if (ii>nested_grid_start_x .and. ii< nested_grid_end_x) then\n",
    "            dx1(i) = dxgrid_nest\n",
    "        else\n",
    "            dx1(i) = dxgrid_orig\n",
    "        end if\n",
    "    end do\n",
    "```            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Size of the subdomains\n",
    "\n",
    "The main issue to address is the size of each subdomain. It is essential that no process runs both a nested and orig part of the grid. \n",
    "\n",
    "So for example if we have `4x4` processes and a `300x300` grid then clearly each process is `75x75`. So the nested domain can be only the inside `4x4`.\n",
    "\n",
    "Conversely, if we require that: \n",
    "\n",
    "- the nested domain is `1km x 4km`, `2m` spacing\n",
    "- the orig domain is `3km x 12km`, `4m` spacing\n",
    "- the nested domain starts at `1km` from the left edge of the orig domain\n",
    "- the nested domain starts at `1100m` from the bottom edge of the orig domain\n",
    "\n",
    "Then that means that we need processes so that `500 x 2000` points are covered but also `150` points. In practice this means we *must* extend the bottom edge so that the distance becomes a multiple of the smallest dimension of the nested domain, such that we satisfy:\n",
    "\n",
    "    rest_x = orig_grid_x-(nested_grid_x*dxgrid_nest))/dxgrid_orig - nested_grid_start_x\n",
    "    rest_y = orig_grid_y-(nested_grid_y*dygrid_nest))/dygrid_orig - nested_grid_start_y    \n",
    "    \n",
    "    n_nest_y*dy = nested_grid_y ! 500 ! 1000m\n",
    "    n_offset_y*dy = nested_grid_start_y ! 150 ! 600m\n",
    "    n_rest_y*dy = rest_y ! 350 ! 1400m    rest_y = \n",
    "    n_nest_x*dx = nested_grid_x ! 4000m\n",
    "    n_offset_x*dx = nested_grid_start_x !250 ! 1000m\n",
    "    n_rest_x*dx = rest_x ! 1750 ! 7000m        \n",
    "    \n",
    "    (n_nest_y+n_offset_y+n_rest_y)*(n_nest_x+n_offset_x+n_rest_x)<=n_max\n",
    "\n",
    "As a first guess say \n",
    "\n",
    "    n_nest_y*n_nest_x = n_max/2 and say that it is 100 = 5*20\n",
    "    \n",
    "Then \n",
    "\n",
    "    dy = 500/5, dx = 2000/20\n",
    "    \n",
    "Then \n",
    "\n",
    "    600m = 150 points => n_offset_y = 3\n",
    "    \n",
    "And \n",
    "\n",
    "    n_offset_x = 250/50 = 5\n",
    "\n",
    "So then we'll have 5x3 + 5x5 + rest_y + 20x5 +20x3 + rest_x\n",
    "\n",
    "And now we must formalise this:\n",
    "\n",
    "- Given an absolute max for the number of processes, say 256\n",
    "- We must find values for all the unknowns above to satisfy the 6 equations and 1 inequality\n",
    "- We turn that last one into an equality for every possible value of n < n_max \n",
    "- That gives us an equation in dx and dy. We can simplify further by setting \n",
    "\n",
    "    dx = dy* (nested_grid_x / nested_grid_y / r) = b*dy/r\n",
    "\n",
    "    where `r` id a fudge factor which change from 0.5 to 2 in steps of 0.5\n",
    "    \n",
    "By substitution and reworking the above equations we obtain an expression for `dy` and consequently for `dx` and `n_max`:\n",
    "\n",
    "    dy = sqrt(a/n_max)\n",
    "    dx = b*dy/r\n",
    "\n",
    "Better?\n",
    "\n",
    "    dy*dy = a/n_max\n",
    "    dy*dx = b*a/n_max/r \n",
    "    \n",
    "In code this looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt,floor\n",
    "\n",
    "def find_nprocs_subgrids(grid_setup,np_max):\n",
    "    nested_grid_x = grid_setup['nested_grid_x']\n",
    "    nested_grid_y = grid_setup['nested_grid_y']\n",
    "    orig_grid_x = grid_setup['orig_grid_x']\n",
    "    orig_grid_y = grid_setup['orig_grid_y']\n",
    "    dxgrid_orig = grid_setup['dxgrid_orig']\n",
    "    dygrid_orig = grid_setup['dygrid_orig']\n",
    "    dxgrid_nest = grid_setup['dxgrid_nest']\n",
    "    dygrid_nest = grid_setup['dygrid_nest']\n",
    "    nested_grid_start_x = grid_setup['nested_grid_start_x']\n",
    "    nested_grid_start_y = grid_setup['nested_grid_start_y']    \n",
    "    print('Possible solutions with number of processes (nprocs) and grid sizes (ip,jp):')\n",
    "    print('') \n",
    "    for np in range(np_max,0,-1):\n",
    "        b = nested_grid_y / nested_grid_x\n",
    "        a_x = (nested_grid_x*(1-dxgrid_nest/dxgrid_orig) + orig_grid_x)        \n",
    "        a_y = (nested_grid_y*(1-dygrid_nest/dygrid_orig) + orig_grid_y)\n",
    "        a_xy = a_x*a_y\n",
    "        a2 = a_xy/np\n",
    "#         print(np,a_x,a_y,a2)\n",
    "        if (a2==floor(a2)):   \n",
    "#             print('Candidate:',a2, ' for np=',np)\n",
    "            b_n = nested_grid_y / nested_grid_x     \n",
    "            b_o = orig_grid_y / orig_grid_x\n",
    "            if b_n < 1:\n",
    "                b_n = 1/b_n\n",
    "            if b_o < 1: \n",
    "                b_o = 1/b_o\n",
    "            b = b_n\n",
    "            if b_o > b:\n",
    "                b = b_o\n",
    "            if b<2:\n",
    "                b=2                \n",
    "            ip_guess = sqrt(a2)\n",
    "            ip_b = floor(ip_guess/b)\n",
    "            ip_e = int(ip_guess*b)\n",
    "            for ip in range(ip_b,ip_e):\n",
    "                jp = a2/ip\n",
    "                nprocs_x = nested_grid_start_x/ip\n",
    "                nprocs_y = nested_grid_start_y/jp\n",
    "                rprocs_x = (orig_grid_x-(nested_grid_x*dxgrid_nest)/dxgrid_orig - nested_grid_start_x)/ip\n",
    "                rprocs_y = (orig_grid_y-(nested_grid_y*dygrid_nest)/dygrid_orig - nested_grid_start_y)/jp\n",
    "                if (jp==floor(jp)):\n",
    "#                     print(np,' Valid dy:',dx,int(dy),';',nx,ny,rx,ry)\n",
    "                        p_col = (orig_grid_x + nested_grid_x*(1 - dxgrid_nest/dxgrid_orig) ) / ip\n",
    "                        p_row = (orig_grid_y + nested_grid_y*(1 - dygrid_nest/dygrid_orig) ) / jp\n",
    "                        if (nprocs_x==floor(nprocs_x)) and (nprocs_y==floor(nprocs_y)) and \\\n",
    "                            (rprocs_x==floor(rprocs_x)) and (rprocs_y==floor(rprocs_y)) and \\\n",
    "                            (p_row==floor(p_row)) and (p_col==floor(p_col)) :\n",
    "                            n_procs_nested_grid = int(p_col-nprocs_x-rprocs_x)*int(p_row-nprocs_y-rprocs_y)\n",
    "                            n_procs_orig_grid = int(np - n_procs_nested_grid)\n",
    "                            print('Number of processes, ip x jp, proc_cols x proc_rows') #, npx, npy')   \n",
    "                            print(np,',',ip,'x',int(jp),',',int(p_col),'x',int(p_row))\n",
    "                            print('procs in orig grid; (before+after) x (below+above) nested grid')\n",
    "                            print(n_procs_orig_grid,';(',int(nprocs_x),'+',int(rprocs_x),')x(',int(nprocs_y),'+',int(rprocs_y),')')\n",
    "                            print('procs in nested grid:')\n",
    "                            print(n_procs_nested_grid,';',int(p_col-nprocs_x-rprocs_x),'x',int(p_row-nprocs_y-rprocs_y))\n",
    "                            o2n = sqrt(dxgrid_orig*dygrid_orig/dxgrid_nest/dygrid_nest)\n",
    "                            print('Number of cores needed:',int(n_procs_nested_grid+n_procs_orig_grid/o2n))\n",
    "                            print('')\n",
    "                            \n",
    "# So now we need some heuristic to group processes from the orig grid.\n",
    "# We assume that we can put sqrt(dxgrid_orig*dygrid_orig/dxgrid_nest/dygrid_nest) processes per core \n",
    "# because they are slower\n",
    "# So in first instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible solutions with number of processes (nprocs) and grid sizes (ip,jp):\n",
      "\n",
      "Number of processes, ip x jp, proc_cols x proc_rows\n",
      "16 , 100 x 100 , 4 x 4\n",
      "procs in orig grid; (before+after) x (below+above) nested grid\n",
      "12 ;( 1 + 1 )x( 1 + 1 )\n",
      "procs in nested grid:\n",
      "4 ; 2 x 2\n",
      "Number of cores needed: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_setup = {\n",
    "'nested_grid_x' : 200,\n",
    "'nested_grid_y' : 200,\n",
    "'nested_grid_start_x' : 100,\n",
    "'nested_grid_start_y' : 100,#75, \n",
    "'orig_grid_x' : 300,\n",
    "'orig_grid_y' : 300,\n",
    "'dxgrid_orig' : 4,\n",
    "'dygrid_orig' : 4,\n",
    "'dxgrid_nest' : 2,\n",
    "'dygrid_nest' : 2\n",
    "}\n",
    "\n",
    "np_max  = 24 \n",
    "                   \n",
    "find_nprocs_subgrids(grid_setup, np_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grid_setup = {\n",
    "'nested_grid_x' : 100,\n",
    "'nested_grid_y' : 100,\n",
    "'nested_grid_start_x' : 100,\n",
    "'nested_grid_start_y' : 100,#75, \n",
    "'orig_grid_x' : 300,\n",
    "'orig_grid_y' : 300,\n",
    "'dxgrid_orig' : 4,\n",
    "'dygrid_orig' : 4,\n",
    "'dxgrid_nest' : 4,\n",
    "'dygrid_nest' : 4\n",
    "}\n",
    "\n",
    "np_max  = 24 \n",
    "                   \n",
    "find_nprocs_subgrids(grid_setup, np_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grid_setup_test = {\n",
    "'nested_grid_x' : 4000/2,\n",
    "'nested_grid_y' : 1000/2,\n",
    "'nested_grid_start_x' : 1000/4,\n",
    "'nested_grid_start_y' : 800/4,\n",
    "'orig_grid_x' : 12000/4,\n",
    "'orig_grid_y' : 2600/4,\n",
    "'dxgrid_orig' : 4,\n",
    "'dygrid_orig' : 4,\n",
    "'dxgrid_nest' : 2,\n",
    "'dygrid_nest' : 2\n",
    "}\n",
    "np_max = 256\n",
    "find_nprocs_subgrids(grid_setup_test, np_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gain compared to using finest grid everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nesting_gain(grid_setup,np_nest,dx,dy):\n",
    "    nested_grid_x = grid_setup['nested_grid_x']\n",
    "    nested_grid_y = grid_setup['nested_grid_y']\n",
    "    orig_grid_x = grid_setup['orig_grid_x']\n",
    "    orig_grid_y = grid_setup['orig_grid_y']\n",
    "    dxgrid_orig = grid_setup['dxgrid_orig']\n",
    "    dygrid_orig = grid_setup['dygrid_orig']\n",
    "    dxgrid_nest = grid_setup['dxgrid_nest']\n",
    "    dygrid_nest = grid_setup['dygrid_nest']\n",
    "    nested_grid_start_x = grid_setup['nested_grid_start_x']\n",
    "    nested_grid_start_y = grid_setup['nested_grid_start_y']\n",
    "    # number of grid points with nesting\n",
    "    a_x = (nested_grid_x*(1-dxgrid_nest/dxgrid_orig) + orig_grid_x)        \n",
    "    a_y = (nested_grid_y*(1-dygrid_nest/dygrid_orig) + orig_grid_y)\n",
    "    # number of grid points without nesting\n",
    "    b_x = orig_grid_x*dxgrid_orig/dxgrid_nest\n",
    "    b_y = orig_grid_y*dygrid_orig/dygrid_nest\n",
    "    np_full_hires = int(b_x*b_y/(dx*dy))\n",
    "#     np*(b_x*b_y/(a_x*a_y)-1)\n",
    "    print('Nested grid requires',np_full_hires - np_nest ,'fewer processes (',int(1000*(1-np_nest/np_full_hires))/10,'%) than full hi-res grid (',np_nest,' i.o.',np_full_hires,')' )\n",
    "    \n",
    "nesting_gain(grid_setup,128,125, 250)    \n",
    "nesting_gain(grid_setup_test,36,75,75)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Populating the `dx` and `dy` arrays per subdomain\n",
    "\n",
    "The `dx` and `dy` arrays per subdomain are used essentially for linear interpolation. They have halos. At the boundaries between orig and nested grids, these halos should contain `dx` and `dy` for the 'other' grid. So we need to detect first if a neighbour left and right is orig or nest. We do this using `inNestedGridByCoord`, and test what grid the neighbours are in.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SCons build script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path # for getting environmental variables on the system\n",
    "\n",
    "# Importing OclBuilder, this is not required for ocl=0\n",
    "# If you want to build just the Fortran code without OpenCL support, use SConstruct.F95_only\n",
    "import OclBuilder\n",
    "from OclBuilder import initOcl\n",
    "\n",
    "# Adding path to includes for kernels\n",
    "CWD= os.environ['PWD']\n",
    "OclBuilder.kopts='-cl-mad-enable -cl-fast-relaxed-math -I'+CWD+'/../OpenCL/Kernels/'\n",
    "\n",
    "from OclBuilder import getOpt\n",
    "OclBuilder.opts=Variables()\n",
    "envF=Environment(useF=1)\n",
    "envF=Environment(ENV={'PATH' : os.environ['PATH']})\n",
    "\n",
    "\n",
    "# Basically, it's Linux unless it's OS X\n",
    "if os.uname()[0] == \"Darwin\":\n",
    "        OSX=1\n",
    "        OSFLAG='-DOSX'\n",
    "else:\n",
    "        OSX=0\n",
    "        OSFLAG='-D__LINUX__'\n",
    "\n",
    "\n",
    "# Then build the rest of the code\n",
    "verbose = getOpt('v','Verbose','1')\n",
    "other=''\n",
    "WITH_OCL=''\n",
    "with_ocl= getOpt('ocl','Use OpenCL','0')\n",
    "if with_ocl == '1':\n",
    "    WITH_OCL = '-D_OPENCL_LES_WV'\n",
    "    envF=initOcl(envF)\n",
    "    kernel_opts = envF['KERNEL_OPTS']\n",
    "else:\n",
    "    envF['F95']=os.environ['FC']\n",
    "    envF['LINK']=os.environ['FC']\n",
    "    VERBOSE = '-DVERBOSE'\n",
    "    if verbose == '0':\n",
    "        VERBOSE = ''\n",
    "    other = getOpt('D','Other macro','')\n",
    "    TIMINGS=''\n",
    "    if other !='':\n",
    "        TIMINGS = '-D'+other\n",
    "\n",
    "# GR: MPI specific build config\n",
    "WITH_MPI=''\n",
    "PROC_PER_ROW=''\n",
    "PROC_PER_COL=''\n",
    "USE_NETCDF_OUTPUT=''\n",
    "with_mpi = getOpt('mpi','Use MPI','0')\n",
    "with_nesting = getOpt('nested','Nested Grid','0')\n",
    "if with_mpi == '1':\n",
    "    procPerRow= getOpt('procPerRow', 'Processes Per Row', '2')\n",
    "    procPerCol= getOpt('procPerCol', 'Processes Per Col', '2')\n",
    "    PROC_PER_ROW = '-DPROC_PER_ROW=' + procPerRow\n",
    "    PROC_PER_COL = '-DPROC_PER_COL=' + procPerCol\n",
    "    WITH_MPI = '-DMPI'\n",
    "    USE_NETCDF_OUTPUT='-DUSE_NETCDF_OUTPUT'\n",
    "#    envF['F95']='mpiifort'\n",
    "#    envF['F95']='mpif90'\n",
    "    if OSX==0:    \n",
    "        envF['LINK']=envF['F95']\n",
    "        envF.Append(LIBS=['netcdf']) # for version less than 4.2.0\n",
    "    else:\n",
    "        envF['F95']=os.environ['FC']\n",
    "        envF['LINK']=os.environ['FC']\n",
    "        envF.Append(LIBS=['mpi_mpifh','netcdff']) # for version more than and equal to 4.2.0 \n",
    "else:\n",
    "    if OSX==1:    \n",
    "        envF['F95']=os.environ['FC']\n",
    "        envF['LINK']=os.environ['FC']\n",
    "        envF.Append(LIBS=['netcdff']) # for version more than and equal to 4.2.0 \n",
    "\n",
    "NESTED_LES=''\n",
    "if with_nesting=='1':\n",
    "\tNESTED_LES='-DNESTED_LES'\n",
    "\t\t\n",
    "GR_DEBUG=''\n",
    "gr_debug = getOpt('gr_debug', 'GR Debug', '0')\n",
    "if gr_debug == '1':\n",
    "    GR_DEBUG='-DGR_DEBUG'\n",
    "\n",
    "WV_DEBUG=''\n",
    "wv_debug = getOpt('wv_debug', 'WV Debug', '0')\n",
    "if wv_debug =='1':\n",
    "    WV_DEBUG='-DMPI_NEW_WV'\n",
    "\n",
    "NO_FILE_IO='-DNO_FILE_IO'\n",
    "ICAL = '-DICAL=0'\n",
    "IFBF='-DIFBF=1'\n",
    "IANIME='-DIANIME=1'\n",
    "IADAM='-DIADAM=0'\n",
    "FFLAGS  = [USE_NETCDF_OUTPUT, WITH_MPI, NESTED_LES, GR_DEBUG, WV_DEBUG, PROC_PER_ROW, PROC_PER_COL, WITH_OCL, NO_FILE_IO, ICAL, IFBF,IANIME, IADAM]\n",
    "\n",
    "if with_ocl == '0':\n",
    "#    FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium', VERBOSE,TIMINGS]\n",
    "#     FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium', '-fno-range-check','-fbounds-check','-Wuninitialized','-ffpe-trap=invalid,zero,overflow', VERBOSE,TIMINGS]\n",
    "     FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium','-fbounds-check', VERBOSE,TIMINGS]\n",
    "#    FFLAGS += ['-cpp','-Ofast', '-m64', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', VERBOSE,TIMINGS]\n",
    "\n",
    "csources=[]\n",
    "CC= os.environ['CC']\n",
    "envC=Environment(CC=CC)\n",
    "if csources:\n",
    "    envC.Library('csubs',csources)\n",
    "\n",
    "fsources = []\n",
    "\n",
    "if with_mpi == '1':\n",
    "    fsources += ['./communication_common.f95', './communication_helper.f95', './communication_helper_integer.f95', './communication_helper_mpi.f95', './communication_helper_real.f95']\n",
    "    \n",
    "if with_nesting == '1':\n",
    "    fsources += ['./nesting_support.f95']\n",
    "\n",
    "if USE_NETCDF_OUTPUT != '':\n",
    "    fsources += ['./module_LES_write_netcdf.f95']\n",
    "\n",
    "fsources+= ['./fortran_helper.f95', './anime.f95','./aveflow.f95','./bondFG.f95','./bondv1.f95','./boundp.f95','./boundsm.f95','./vel2.f95','./velFG.f95','./feedbf.f95','./feedbfm.f95','./les.f95','./grid.f95','./ifdata.f95','./init.f95','./main.f95','./set.f95','./timdata.f95','./common_sn.f95','./params_common_sn.f95']\n",
    "\n",
    "ffsources=[]\n",
    "\n",
    "if with_ocl == '1':\n",
    "    ffsources = fsources + ['./module_LES_conversions.f95','./module_LES_combined_ocl.f95','./oclWrapper.o']\n",
    "else:\n",
    "    ffsources = fsources + ['./adam.f95','./press.f95','./velnw.f95']\n",
    "\n",
    "\n",
    "if with_ocl == '1':\n",
    "    # Linker flags for OclWrapper\n",
    "    OPENCL_DIR=os.environ['OPENCL_DIR']\n",
    "    OCL_LDFLAGS =  ['-L.','-L'+OPENCL_DIR+'/OpenCLIntegration']\n",
    "else:\n",
    "    OCL_LDFLAGS =  []\n",
    "\n",
    "if OSX == 1:\n",
    "# Assuming MacPorts\n",
    "    INCLPATH = ['/opt/local/include','/opt/local/include/openmpi-gcc49/','/opt/local/lib/openmpi-gcc49/']\n",
    "    LIBPATH = ['/opt/local/lib','/opt/local/lib/openmpi-gcc49/']\n",
    "else:\n",
    "# test for devtoolset-2 ... so better use a var $DEVTOOLSETROOT?\n",
    "    if os.path.exists('/opt/rh/devtoolset-2'):\n",
    "        INCLPATH = ['/opt/rh/devtoolset-2/root/usr/include' ]\n",
    "        LIBPATH = '/opt/rh/devtoolset-2/root/usr/lib'\n",
    "    else:\n",
    "# reasonable default ...\n",
    "        NETCDF = os.environ.get('NETCDF_DIR')\n",
    "        INCLPATH = [NETCDF + '/include']\n",
    "        LIBPATH  = [NETCDF + '/lib']\n",
    "#MPICH = os.environ.get('MPICH')\n",
    "#INCLPATH = [NETCDF + '/include', MPICH + '/include']\n",
    "#LIBPATH = [NETCDF + '/lib', MPICH + '/lib']\n",
    "#        INCLPATH = [NETCDF + '/include', '/usr/include']\n",
    "#        LIBPATH  = [NETCDF + '/lib', '/usr/local/lib']\n",
    "#        INCLPATH = ['/usr/local/include', '/usr/include' ]\n",
    "#        LIBPATH = '/usr/local/lib'\n",
    "#INCLPATH += ['../OpenCL','../OpenCL/Wrappers']\n",
    "\n",
    "envF.Append(F95FLAGS=FFLAGS)\n",
    "envF.Append(F95PATH=['.',INCLPATH])\n",
    "envF.Append(LIBPATH=['.',LIBPATH])\n",
    "if OSX != 1:\n",
    "    envF.Append(LIBS=['m'])\n",
    "\n",
    "mpi_ext=''\n",
    "if with_mpi == '1':\n",
    "    mpi_ext='_mpi'\n",
    "\n",
    "ocl_ext=''\n",
    "if with_ocl == '1':\n",
    "    envF.Append(LIBS=['OclWrapperF','stdc++','OclWrapper'])\n",
    "    ocl_ext='_ocl'\n",
    "    if OSX==1:\n",
    "            envF.Append(FRAMEWORKS=['OpenCL'])\n",
    "    else:\n",
    "            envF.Append(LIBS=['OpenCL'])\n",
    "\n",
    "if csources:\n",
    "    envF.Append(LIBS=['csubs'])\n",
    "\n",
    "prog = envF.Program('les_main'+ocl_ext+mpi_ext,ffsources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helper script to expand a portion of a GIS file by extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%perl\n",
    "#!/usr/bin/env perl\n",
    "use v5.24;\n",
    "use warnings;\n",
    "use strict;\n",
    "\n",
    "my %nd = (\n",
    " nested_grid_start_x => 100,\n",
    " nested_grid_start_y => 100,\n",
    " nested_grid_x => 200,\n",
    " nested_grid_y => 200,\n",
    " orig_grid_x => 300,\n",
    " orig_grid_y => 300,\n",
    " dxgrid_nest => 2,\n",
    " dygrid_nest => 2,\n",
    " dxgrid_orig => 4,\n",
    " dygrid_orig => 4,\n",
    ");\n",
    "\n",
    "my $VIS=0;\n",
    "\n",
    "if (not @ARGV) {\n",
    "    die \"$0 GIS_file\\n\";\n",
    "}\n",
    "my $gis_file = $ARGV[0];\n",
    "my $file_name = $gis_file;\n",
    "$file_name=~s/\\.txt$//;\n",
    "\n",
    "my $row_size = $nd{orig_grid_x};\n",
    "\n",
    "open my $IN, '<', $gis_file or die $!;\n",
    "\n",
    "my $domain=[];\n",
    "my $row_ct = 0;\n",
    "my $col_ct = 0;\n",
    "my $push_count=0;\n",
    "\n",
    "while (my $val = <$IN>) {\n",
    "    chomp $val;\n",
    "    if ($col_ct==0) {\n",
    "        $domain->[$row_ct]=[];        \n",
    "    }\n",
    "    if ($col_ct == $row_size) {\n",
    "        $col_ct=0;\n",
    "        $row_ct++;\n",
    "    }\n",
    "#    say \"$row_ct, $col_ct, $val\";\n",
    "    push @{$domain->[$row_ct]}, $val;\n",
    "    $col_ct++;\n",
    "}\n",
    "close $IN;\n",
    "\n",
    "my $n_rows = $row_ct+1;\n",
    "\n",
    "output_dat_file_and_plot($domain) if $VIS;\n",
    "# Now manipulate this\n",
    "\n",
    "\n",
    "\n",
    "my $ratio_x = $nd{dxgrid_orig} / $nd{dxgrid_nest};\n",
    "my $ratio_y = $nd{dygrid_orig} / $nd{dygrid_nest};\n",
    "\n",
    "# First increase the columns\n",
    "my $ndomain=[];\n",
    "for my $row (0 .. $n_rows-1) {\n",
    "    $ndomain->[$row]=[];\n",
    "    for my $col (0 .. $row_size-1) { \n",
    "            my $val =  $domain->[$row][$col];\n",
    "        if ($col >= $nd{nested_grid_start_x} && $col < $nd{nested_grid_start_x} + $nd{nested_grid_x}/$ratio_x) {\n",
    "#            $domain->[$row][$col]=[$val,$val];\n",
    "            @{$ndomain->[$row]} = (@{$ndomain->[$row]}, $val, $val);\n",
    "        } else {\n",
    "            push @{$ndomain->[$row]}, $val;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "output_dat_file_and_plot($ndomain) if $VIS;\n",
    "\n",
    "my $nndomain=[];\n",
    "\n",
    "for my $row (0 .. $n_rows -1) {\n",
    "    push @{$nndomain}, $ndomain->[$row];\n",
    "        if ($row >= $nd{nested_grid_start_y} && $row < $nd{nested_grid_start_y} + $nd{nested_grid_y}/$ratio_y) {\n",
    "            push @{$nndomain}, $ndomain->[$row];\n",
    "        } \n",
    "}\n",
    "\n",
    "output_dat_file_and_plot($nndomain) if $VIS;\n",
    "\n",
    "output_new_GIS($nndomain, \\%nd);\n",
    "\n",
    "sub output_new_GIS { (my $domain, my $nest_def) =@_;\n",
    "    my @ks = sort keys %{$nest_def};\n",
    "    my $nest_str= join('_',map { $nest_def->{$_} } @ks);\n",
    "    open my $GIS,'>', $file_name.'_nest_'.$nest_str.'.txt' or die $!;\n",
    "    my $n_rows = scalar @{$domain};\n",
    "    my $n_cols = scalar @{$domain->[0]};\n",
    "    for my $row (0 .. $n_rows-1) {\n",
    "        for my $col (0 .. $n_cols-1) { \n",
    "            my $val =  $domain->[$row][$col];\n",
    "            say $GIS $val;\n",
    "        }\n",
    "    }\n",
    "    close $GIS;   \n",
    "}\n",
    "\n",
    "sub output_dat_file_and_plot { (my $domain) =@_;\n",
    "    open my $DAT, '>', $file_name.'.dat' or die $!;\n",
    "    my $n_rows = scalar @{$domain};\n",
    "    my $n_cols = scalar @{$domain->[0]};\n",
    "    for my $row (0 .. $n_rows-1) {\n",
    "        for my $col (0 .. $n_cols-1) { \n",
    "            my $val = $domain->[$row][$col];\n",
    "            if (ref($val) eq 'ARRAY') {\n",
    "                $val = join(\"\\t\",@{$val});\n",
    "            }\n",
    "            print $DAT $val,\"\\t\";\n",
    "        }\n",
    "        print $DAT \"\\n\";\n",
    "    }\n",
    "    close $DAT;\n",
    "    open my $GP, '>','tmp.gnuplot';\n",
    "    say $GP \"plot '$file_name.dat' matrix with image\";\n",
    "    close $GP;\n",
    "    system(\"gnuplot tmp.gnuplot\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Synchronisation problems\n",
    "\n",
    "The current approach to synchronisation leads to deadlock in the halo exchanges. There are two easy fixes:\n",
    "\n",
    "1. Run all processes at the smallest timestep\n",
    "2. Only exchange at the largest timestep\n",
    "\n",
    "Neither is satisfactory but at the moment I don't quite know how to fix the deadlock as I do not see how it arises.\n",
    "Option 1. is preferable as it is definitely more correct. But it needs more resources.\n",
    "Option 2. will likely lead to higher numerical errors.\n",
    "\n",
    "### Debugging:\n",
    "\n",
    "* Step 1.\n",
    "\n",
    "    - set both time steps to 0.025 but sync every other step\n",
    "```fortran    \n",
    "!        if (inNest) then\n",
    "        if (.true. .or. inNest) then\n",
    "!            if (mod(n,int(dt_orig/dt_nest))==0) then\n",
    "            if (mod(n,2)==0) then    \n",
    "```    \n",
    "    - only keep bondv1 using macro `#WV_DEBUG_MPI`\n",
    "    - ` ./build_and_run_mac.sh 4 4 'D=TEST_NESTED_LES' `\n",
    "    => seems OK\n",
    "\n",
    "* Step 2.\n",
    "    \n",
    "    - Same but full sim except `anime, ifdata_out, aveflow`\n",
    "    => This does not deadlock but returns NaN\n",
    "    \n",
    "* Step 3. \n",
    "    \n",
    "        - full sim, sync every step\n",
    "```fortran    \n",
    "        if (.false. .or. inNest) then\n",
    "```    \n",
    "    => NaN\n",
    "    \n",
    "* Step 4.     \n",
    "    \n",
    "        - wv_debug=0\n",
    "        => NaN\n",
    "        \n",
    "* Step 5.          \n",
    "\n",
    "        - reverted to sync per timestep, wv_debug=1\n",
    "        => OK\n",
    "         - So the issue is the 'sync every other step'\n",
    "         - Ignore for now, look at the MPI issues\n",
    "         \n",
    "* Step 6.\n",
    "\n",
    "        - Fixed condition in `bondv1`:\n",
    "```fortran        \n",
    "#ifdef NESTED_LES\n",
    "    if(syncTicks == 0 .and. n == 2) then\n",
    "#else\n",
    "    if(n == 1) then\n",
    "#endif\n",
    "```endif\n",
    "        - This does not fix the NaN\n",
    "        \n",
    "* Step 7. \n",
    "\n",
    "    - Moved creation of communicator for bottom row to main, do only once        \n",
    "    - Check if syncTicks is used as a guard everywhere, this is OK\n",
    "    - Now go back to only `bondv1`, test the actual sync logic, but with identical timesteps\n",
    "    => Lo and behold, this does not crash or hang\n",
    "    \n",
    "* Step 8. \n",
    "\n",
    "    - Only `bondv1`, test the actual sync logic with differnt time steps\n",
    "\n",
    "           0.00000000    \n",
    "                   1           1           9   0.00000000       0.00000000       0.00000000    \n",
    "                   1           1          10   0.00000000       0.00000000       0.00000000    \n",
    "                   1           1           5   0.00000000       0.00000000       0.00000000    \n",
    "                   1           1           6   0.00000000       0.00000000       0.00000000    \n",
    "        [h144:61665] *** An error occurred in MPI_Waitall\n",
    "        [h144:61665] *** reported by process [342097921,9]\n",
    "        [h144:61665] *** on communicator MPI_COMMUNICATOR 3\n",
    "        [h144:61665] *** MPI_ERR_TRUNCATE: message truncated\n",
    "        [h144:61665] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
    "        [h144:61665] ***    and potentially your MPI job)\n",
    "        [h144.85.244.10.31339.vlan.kuins.net:61655] 3 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal\n",
    "        [h144.85.244.10.31339.vlan.kuins.net:61655] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
    "\n",
    "    - Restrict even further, only halos\n",
    "        => Still crashes\n",
    "        \n",
    "* Step 9.\n",
    "    \n",
    "    - I changed the halo exchange to happen only for n>2. The crash disappears but after 12 timesteps (time = 0.55)the system deadlocks. \n",
    "    - Commenting corner halos still deadlocks\n",
    "    - MPI_Barrier after halo exchange makes no difference    \n",
    "    - Found it: I forgot to adapt the number of steps so that slow and fast processes match! Now it finishes!\n",
    "\n",
    "* Step 10.\n",
    "\n",
    "    - Running all but `press` and the region guarded by `IANIME`\n",
    "    => Crash\n",
    "    - OK, need to guard all halo exchanges \n",
    "```fortran        \n",
    "#ifdef NESTED_LES\n",
    "       if (syncTicks == 0  .and. n > 2) then\n",
    "#endif\n",
    "```\n",
    "    - So needed to add `n` as arg to `velFG`, `vel2` and `boundsm` (called from `les`)\n",
    "    \n",
    "* Step 11. \n",
    "\n",
    "    - With `press`\n",
    "    - So needed to add `n` as arg to `bondfg`, `boundp1` and `boundp2` (called from `press`)\n",
    "    - Required to change the calls to `boundp1` and `boundp2` in `ifdata` as well.\n",
    "    \n",
    "* Step 12.\n",
    "\n",
    "    - I had to guard the global operations as well. So instead I put a 'n > 2' condition in `main`, that seems to work well.\n",
    "    - So it works now!\n",
    "    - But when I turn on `anime` it hangs, same reason. Fixed\n",
    "    \n",
    "So, in summary, the LES with nested grid now builds and runs correctly as far as I can tell, and the I/O features are supported for the code that I have. The is still the file `bondv1_data24` which I can't test as it is not part of my source tree, but I guess I can already change it similar to `bondv     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
